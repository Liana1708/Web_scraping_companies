{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Chrome('C:/Users/MikasaServer/Downloads/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames = ['Company']\n",
    "companies_df = pd.read_csv('Companies_names.csv', encoding = \"ISO-8859-1\", names=colnames)\n",
    "\n",
    "#We import as 'companies' the companies names.\n",
    "\n",
    "companies = companies_df['Company']\n",
    "companies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pages to avoid :\n",
    "\n",
    "#https://beta.companieshouse.gov.uk\n",
    "#https://en.wikipedia.org\n",
    "#https://www.theconstructionindex.co.uk\n",
    "#https://www.bloomberg.com\n",
    "#https://www.linkedin.com\n",
    "#https://companycheck.co.uk\n",
    "\n",
    "#We store partial links of the wrong websites in a list:\n",
    "wrong_websites=['Officers','Overview', 'LinkedIn','Wikipedia', 'Free business summary', 'Endole','Bloomberg', 'theconstructionindex', 'SoloCheck', 'Vision-Net']\n",
    "\n",
    "#We store the possible partial links of the document in a list:\n",
    "documents_names=[X]\n",
    "\n",
    "len(wrong_websites)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "webpage=[]\n",
    "documents=[]\n",
    "no_documents=[]\n",
    "\n",
    "for i in np.arange(1612):\n",
    "    browser.get(\"https://www.google.com/\")\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    input=browser.find_elements_by_tag_name('input')[3].send_keys(companies[i], Keys.RETURN)\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    \n",
    "    try :\n",
    "        website= browser.find_element_by_partial_link_text('.co.uk')\n",
    "        try:\n",
    "            for i in np.arange(10):\n",
    "                wrong=browser.find_element_by_partial_link_text(wrong_websites[i])\n",
    "        except NoSuchElementException:\n",
    "            wrong=None\n",
    "        if not website is wrong:                                    \n",
    "            print(website.text)\n",
    "            webpage.append(website.text)\n",
    "            website.click()\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        website= browser.find_element_by_partial_link_text('.com')\n",
    "        try:\n",
    "            for i in np.arange(10):\n",
    "                wrong=browser.find_element_by_partial_link_text(wrong_websites[i])\n",
    "        except NoSuchElementException:\n",
    "            wrong=None\n",
    "        if not website is wrong:\n",
    "            try:\n",
    "                print(website.text)\n",
    "                webpage.append(website.text)\n",
    "                website.click()   \n",
    "            except NoSuchElementException:\n",
    "                next\n",
    "                \n",
    "    time.sleep(3)\n",
    "    #This is to find a particular document inside the webpage: \n",
    "    \n",
    "    try:\n",
    "        for r in document_names:\n",
    "            document=browser.find_element_by_partial_link_text(r)\n",
    "            document=document.get_attribute(\"href\")\n",
    "            print(document\n",
    "            documents.append(document)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        no_document='No document found'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames = ['Company']\n",
    "companies_df = pd.read_csv('Companies_names_2_for_scraper.csv', encoding = \"ISO-8859-1\", names=colnames)\n",
    "\n",
    "#We import as 'companies' the companies names.\n",
    "\n",
    "companies_2 = companies_df['Company']\n",
    "companies_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df = pd.DataFrame(data=webpage)\n",
    "df.to_csv(\"2_webpages_result.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=documents)\n",
    "df.to_csv(\"2_documents_result.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=no_document)\n",
    "df.to_csv(\"1_no_documents_result.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "companies_2.head()\n",
    "len(companies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage=[]\n",
    "documents=[]\n",
    "\n",
    "\n",
    "for i in np.arange(521):\n",
    "    browser.get(\"https://www.google.com/\")\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    input=browser.find_elements_by_tag_name('input')[3].send_keys(companies_2[i], Keys.RETURN)\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    \n",
    "    try :\n",
    "        website= browser.find_element_by_partial_link_text('.co.uk')\n",
    "        try:\n",
    "            for i in np.arange(10):\n",
    "                wrong=browser.find_element_by_partial_link_text(wrong_websites[i])\n",
    "        except NoSuchElementException:\n",
    "            wrong=None\n",
    "        if not website is wrong:                                    \n",
    "            print(website.text)\n",
    "            webpage.append(website.text)\n",
    "            website.click()\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        website= browser.find_element_by_partial_link_text('.com')\n",
    "        try:\n",
    "            for i in np.arange(10):\n",
    "                wrong=browser.find_element_by_partial_link_text(wrong_websites[i])\n",
    "        except NoSuchElementException:\n",
    "            wrong=None\n",
    "        if not website is wrong:\n",
    "            try:\n",
    "                print(website.text)\n",
    "                webpage.append(website.text)\n",
    "                website.click()   \n",
    "            except NoSuchElementException:\n",
    "                next\n",
    "                \n",
    "    time.sleep(3)\n",
    "    #This is to find the document inside the webpage: \n",
    "    \n",
    "    try:\n",
    "        for r in documents_names:\n",
    "            document=browser.find_element_by_partial_link_text(r)\n",
    "            document=document.get_attribute(\"href\")\n",
    "            print(document)\n",
    "            documents.append(document)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        no_document='No document found'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified code to try later \n",
    "#In documents scraper include the words: View, Here, view, here. \n",
    "\n",
    "webpage_2=[]\n",
    "documents_2=[]\n",
    "\n",
    "\n",
    "for i in np.arange(1):\n",
    "    browser.get(\"https://www.google.com/\")\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    input=browser.find_elements_by_tag_name('input')[3].send_keys(companies_2[i], Keys.RETURN)\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "    \n",
    "    try :\n",
    "        website_1= browser.find_element_by_partial_link_text('.co.uk')\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "             website_2= browser.find_element_by_partial_link_text('.com')\n",
    "        except NoSuchElementException:\n",
    "            None\n",
    "    \n",
    "    for i in np.arange(10):\n",
    "            try:\n",
    "                wrong=browser.find_element_by_partial_link_text(wrong_websites[i])\n",
    "            except NoSuchElementException:\n",
    "                wrong=None\n",
    "    if not website_1 is wrong:\n",
    "        print(website_1.text)\n",
    "        webpage_2.append(website_1.text)\n",
    "        website_1.click()\n",
    "\n",
    "        \n",
    "    if not website_2 is wrong: \n",
    "        print(website_2.text)\n",
    "        webpage_2.append(website_2.text)\n",
    "        website_2.click()\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
